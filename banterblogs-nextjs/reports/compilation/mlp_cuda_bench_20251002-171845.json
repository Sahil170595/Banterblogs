{ "generated_at": "2025-10-02T21:20:00.971458+00:00", "model": { "name": "mlp", "device": "cuda", "dtype": "torch.float32", "batch_size": 4, "input_dim": 256, "hidden_dim": 512 }, "settings": { "runs": 10, "warmup_runs": 3, "backends": [ "eager", "jit", "torch_compile", "onnx", "tensorrt" ] }, "backends": { "eager": { "backend": "eager", "compilation": null, "benchmark": { "backend": "eager", "mean_time_ms": 0.2407400002994109, "std_time_ms": 0.046098984962244405, "min_time_ms": 0.21629999901051633, "max_time_ms": 0.3766000008909032, "metadata": { "compile_time_s": 0.0 } } }, "jit": { "backend": "jit", "compilation": { "backend": "torchscript", "success": true, "compile_time_s": 0.37795030000052066, "metadata": { "strict": false, "frozen": true, "graph_preview": [ "graph(%self.1 : __torch__.torch.nn.modules.container.___torch_mangle_9.Sequential,", " %input.1 : Tensor):", " %15 : str = prim::Constant[value=\"none\"](), scope: __module.1 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:738:0", " %self.4.weight : Float(256, 512, strides=[512, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.4.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.2.weight : Float(512, 512, strides=[512, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.2.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.0.weight : Float(512, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.0.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %input.3 : Tensor = aten::linear(%input.1, %self.0.weight, %self.0.bias), scope: __module.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0", " %input.5 : Tensor = aten::gelu(%input.3, %15), scope: __module.1 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:738:0", " %input.7 : Tensor = aten::linear(%input.5, %self.2.weight, %self.2.bias), scope: __module.2 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0", " %input : Tensor = aten::gelu(%input.7, %15), scope: __module.3 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:738:0", " %24 : Tensor = aten::linear(%input, %self.4.weight, %self.4.bias), scope: __module.4 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0", " return (%24)" ] }, "error": null }, "benchmark": { "backend": "jit", "mean_time_ms": 0.21373999952629674, "std_time_ms": 0.015051126521162295, "min_time_ms": 0.19070000053034164, "max_time_ms": 0.23969999892869964, "metadata": { "compile_time_s": 0.37795030000052066, "strict": false, "frozen": true, "graph_preview": [ "graph(%self.1 : __torch__.torch.nn.modules.container.___torch_mangle_9.Sequential,", " %input.1 : Tensor):", " %15 : str = prim::Constant[value=\"none\"](), scope: __module.1 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:738:0", " %self.4.weight : Float(256, 512, strides=[512, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.4.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.2.weight : Float(512, 512, strides=[512, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.2.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.0.weight : Float(512, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.0.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %input.3 : Tensor = aten::linear(%input.1, %self.0.weight, %self.0.bias), scope: __module.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0", " %input.5 : Tensor = aten::gelu(%input.3, %15), scope: __module.1 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:738:0", " %input.7 : Tensor = aten::linear(%input.5, %self.2.weight, %self.2.bias), scope: __module.2 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0", " %input : Tensor = aten::gelu(%input.7, %15), scope: __module.3 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:738:0", " %24 : Tensor = aten::linear(%input, %self.4.weight, %self.4.bias), scope: __module.4 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125:0", " return (%24)" ] } } }, "torch_compile": { "backend": "torch_compile", "compilation": { "backend": "torch.compile", "success": false, "compile_time_s": null, "metadata": {}, "error": "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n" }, "benchmark": { "backend": "torch_compile", "mean_time_ms": 0.0, "std_time_ms": 0.0, "min_time_ms": 0.0, "max_time_ms": 0.0, "metadata": { "error": "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n" } } }, "onnx": { "backend": "onnx", "compilation": { "backend": "onnx", "success": true, "compile_time_s": 0.3035955000013928, "metadata": { "onnx_path": "C:\\Users\\sahil\\AppData\\Local\\Temp\\chimera_onnx__efzrtzl\\model.onnx", "opset": 17, "providers": [ "CPUExecutionProvider" ], "session_type": "onnxruntime" }, "error": null }, "benchmark": { "backend": "onnx", "mean_time_ms": 0.04692999973485712, "std_time_ms": 0.0029853147027788206, "min_time_ms": 0.04389999958220869, "max_time_ms": 0.05280000186758116, "metadata": { "compile_time_s": 0.3035955000013928, "onnx_path": "C:\\Users\\sahil\\AppData\\Local\\Temp\\chimera_onnx__efzrtzl\\model.onnx", "opset": 17, "providers": [ "CPUExecutionProvider" ], "session_type": "onnxruntime" } } }, "tensorrt": { "backend": "tensorrt", "compilation": { "backend": "tensorrt", "success": true, "compile_time_s": 40.534541199998785, "metadata": { "enabled_precisions": [ "torch.float16" ], "workspace_size": 268435456 }, "error": null }, "benchmark": { "backend": "tensorrt", "mean_time_ms": 0.4037100010464201, "std_time_ms": 0.2175039750429976, "min_time_ms": 0.25290000121458434, "max_time_ms": 0.7669000005989801, "metadata": { "compile_time_s": 40.534541199998785, "enabled_precisions": [ "torch.float16" ], "workspace_size": 268435456 } } } }
}
{ "generated_at": "2025-10-02T21:21:56.014430+00:00", "model": { "name": "transformer", "device": "cuda", "dtype": "torch.float32", "batch_size": 2, "seq_len": 128, "embed_dim": 256, "num_heads": 4, "num_layers": 2 }, "settings": { "runs": 10, "warmup_runs": 3, "backends": [ "eager", "jit", "torch_compile", "onnx", "tensorrt" ] }, "backends": { "eager": { "backend": "eager", "compilation": null, "benchmark": { "backend": "eager", "mean_time_ms": 0.7961700004671002, "std_time_ms": 0.03181484129266149, "min_time_ms": 0.7565000014437828, "max_time_ms": 0.845300000946736, "metadata": { "compile_time_s": 0.0 } } }, "jit": { "backend": "jit", "compilation": { "backend": "torchscript", "success": true, "compile_time_s": 0.160169799997675, "metadata": { "strict": false, "frozen": true, "graph_preview": [ "graph(%self.1 : __torch__.___torch_mangle_39.TinyTransformer,", " %x : Tensor):", " %10 : NoneType = prim::Constant(), scope: __module.encoder/__module.encoder.layers.0", " %9 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %8 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %7 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %6 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %self.head.weight : Float(256, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.head.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.self_attn.in_proj_weight : Float(768, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.self_attn.in_proj_bias : Float(768, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.self_attn.out_proj.weight : Float(256, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.norm2.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.norm2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear1.weight : Float(1024, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear1.bias : Float(1024, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear2.weight : Float(256, 1024, strides=[1024, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %src : Tensor = aten::_transformer_encoder_layer_fwd(%x, %6, %7, %self.encoder.layers.0.self_attn.in_proj_weight, %self.encoder.layers.0.self_attn.in_proj_bias, %self.encoder.layers.0.self_attn.out_proj.weight, %self.encoder.layers.0.norm2.bias, %8, %8, %9, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.linear1.weight, %self.encoder.layers.0.linear1.bias, %self.encoder.layers.0.linear2.weight, %self.encoder.layers.0.linear2.bias, %10, %10), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %input : Tensor = aten::_transformer_encoder_layer_fwd(%src, %6, %7, %self.encoder.layers.0.self_attn.in_proj_weight, %self.encoder.layers.0.self_attn.in_proj_bias, %self.encoder.layers.0.self_attn.out_proj.weight, %self.encoder.layers.0.norm2.bias, %8, %8, %9, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.linear1.weight, %self.encoder.layers.0.linear1.bias, %self.encoder.layers.0.linear2.weight, %self.encoder.layers.0.linear2.bias, %10, %10), scope: __module.encoder/__module.encoder.layers.1 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0" ] }, "error": null }, "benchmark": { "backend": "jit", "mean_time_ms": 0.808590000087861, "std_time_ms": 0.043301765986921203, "min_time_ms": 0.7439000000886153, "max_time_ms": 0.884000000951346, "metadata": { "compile_time_s": 0.160169799997675, "strict": false, "frozen": true, "graph_preview": [ "graph(%self.1 : __torch__.___torch_mangle_39.TinyTransformer,", " %x : Tensor):", " %10 : NoneType = prim::Constant(), scope: __module.encoder/__module.encoder.layers.0", " %9 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %8 : bool = prim::Constant[value=0](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %7 : int = prim::Constant[value=4](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %6 : int = prim::Constant[value=256](), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %self.head.weight : Float(256, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.head.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.self_attn.in_proj_weight : Float(768, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.self_attn.in_proj_bias : Float(768, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.self_attn.out_proj.weight : Float(256, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.norm2.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.norm2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear1.weight : Float(1024, 256, strides=[256, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear1.bias : Float(1024, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear2.weight : Float(256, 1024, strides=[1024, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %self.encoder.layers.0.linear2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()", " %src : Tensor = aten::_transformer_encoder_layer_fwd(%x, %6, %7, %self.encoder.layers.0.self_attn.in_proj_weight, %self.encoder.layers.0.self_attn.in_proj_bias, %self.encoder.layers.0.self_attn.out_proj.weight, %self.encoder.layers.0.norm2.bias, %8, %8, %9, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.linear1.weight, %self.encoder.layers.0.linear1.bias, %self.encoder.layers.0.linear2.weight, %self.encoder.layers.0.linear2.bias, %10, %10), scope: __module.encoder/__module.encoder.layers.0 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0", " %input : Tensor = aten::_transformer_encoder_layer_fwd(%src, %6, %7, %self.encoder.layers.0.self_attn.in_proj_weight, %self.encoder.layers.0.self_attn.in_proj_bias, %self.encoder.layers.0.self_attn.out_proj.weight, %self.encoder.layers.0.norm2.bias, %8, %8, %9, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.norm2.weight, %self.encoder.layers.0.norm2.bias, %self.encoder.layers.0.linear1.weight, %self.encoder.layers.0.linear1.bias, %self.encoder.layers.0.linear2.weight, %self.encoder.layers.0.linear2.bias, %10, %10), scope: __module.encoder/__module.encoder.layers.1 # C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:902:0" ] } } }, "torch_compile": { "backend": "torch_compile", "compilation": { "backend": "torch.compile", "success": false, "compile_time_s": null, "metadata": {}, "error": "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n" }, "benchmark": { "backend": "torch_compile", "mean_time_ms": 0.0, "std_time_ms": 0.0, "min_time_ms": 0.0, "max_time_ms": 0.0, "metadata": { "error": "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n" } } }, "onnx": { "backend": "onnx", "compilation": { "backend": "onnx", "success": true, "compile_time_s": 0.23457670000061626, "metadata": { "onnx_path": "C:\\Users\\sahil\\AppData\\Local\\Temp\\chimera_onnx_szuyfbwu\\model.onnx", "opset": 17, "providers": [ "CPUExecutionProvider" ], "session_type": "onnxruntime" }, "error": null }, "benchmark": { "backend": "onnx", "mean_time_ms": 3.109000000404194, "std_time_ms": 0.281127849155082, "min_time_ms": 2.7077000013377983, "max_time_ms": 3.5809000000881497, "metadata": { "compile_time_s": 0.23457670000061626, "onnx_path": "C:\\Users\\sahil\\AppData\\Local\\Temp\\chimera_onnx_szuyfbwu\\model.onnx", "opset": 17, "providers": [ "CPUExecutionProvider" ], "session_type": "onnxruntime" } } }, "tensorrt": { "backend": "tensorrt", "compilation": { "backend": "tensorrt", "success": true, "compile_time_s": 21.228892399998585, "metadata": { "enabled_precisions": [ "torch.float16" ], "workspace_size": 268435456 }, "error": null }, "benchmark": { "backend": "tensorrt", "mean_time_ms": 1.1818299994047265, "std_time_ms": 0.1523181931638638, "min_time_ms": 1.0997999997925945, "max_time_ms": 1.627499997994164, "metadata": { "compile_time_s": 21.228892399998585, "enabled_precisions": [ "torch.float16" ], "workspace_size": 268435456 } } } }
}